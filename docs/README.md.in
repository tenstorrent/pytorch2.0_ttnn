[![Ask DeepWiki](https://deepwiki.com/badge.svg)](https://deepwiki.com/tenstorrent/pytorch2.0_ttnn)

# PyTorch 2.0 TTNN Compiler
The PyTorch 2.0 TT-NN Compiler enables seamless execution of PyTorch models on [Tenstorrent](https://tenstorrent.com/) AI accelerators. 
By leveraging the TT-NN backend, you can achieve significant performance improvements while maintaining PyTorch's familiar API.

## ðŸš€ Quick Start

### Installation

**For PyPI users** (recommended):
```bash
pip install torch-ttnn[pypi]
```

**For development** (building from source):

**Important**: TT-Metal is included as a git submodule. The build system **automatically detects** TT-Metal from the submodule and **actively ignores** the `TT_METAL_HOME` environment variable to prevent build conflicts when switching between TT projects.

1. Clone with submodules and build tt-metal:
```bash
git clone --recursive https://github.com/tenstorrent/pytorch2.0_ttnn.git
cd pytorch2.0_ttnn/torch_ttnn/cpp_extension/third-party/tt-metal
./build_metal.sh --release --enable-ccache
./create_venv.sh
source python_env/bin/activate
```

2. Install pytorch2.0_ttnn:
```bash
cd ../../../../  # Return to pytorch2.0_ttnn root
pip install --upgrade pip scikit-build-core cmake ninja
pip install -e .[dev]
```

> **ðŸ“– Detailed Instructions:** See [docs/BuildFlow.md](docs/BuildFlow.md) for complete build documentation and troubleshooting.

**Note**: The `[pypi]` extra is required for PyPI users to install the `ttnn` runtime dependency. Development builds use the locally built ttnn from the submodule.

### âœ¨ Basic Usage

**Option 1: Eager Mode:** get your model running by switching to a TT device
```python
import torch
import torch_ttnn

model = YourModel()

device = ttnn.open_device(device_id=0)
model.to(torch_ttnn.ttnn_device_as_torch_device(device))

output = model(input_data)
```

**Option 2: Compilation Mode (Recommended):** get more perf with a JIT compiler
```python
import torch
import torch_ttnn

model = YourModel()

device = ttnn.open_mesh_device(ttnn.MeshShape(1, 2))  # 1x2 device grid
option = torch_ttnn.TorchTtnnOption(device=device, data_parallel=2)

model = torch.compile(model, backend=torch_ttnn.backend, options=option)
output = model(input_data)
```

## ðŸ“Š Model Support

We've extensively tested the compiler across a diverse range of model architectures. Here's a summary of our validation results:

{metrics_md}

### Explanation of Metrics

{explanations_md}

***

# Contributing

Whether you are new to Tenstorrent hardware or an experienced developer, there are many ways to contribute.

## Getting Started

Start with our high level [Contribution guide](docs/Contributing.md).
You can find more information here:
* [Discussions](https://github.com/tenstorrent/pytorch2.0_ttnn/discussions)
* [Operations Report](docs/OperationsReport.md)
* [Lowering TT-NN Operation to PyTorch](docs/AddNewOperationLowering.md)
* [Native Device Integration Extension](docs/OpenRegistrationAPI.md)
* [Build with Metal from Source](docs/DevelopWithMetalFromSources.md)
* [Known Issues](docs/KnownIssues.md)
* [Problem Solving](docs/ProblemSolving.md)

We encourage contributions and offer [ðŸ¤‘ Bounties](https://github.com/tenstorrent/pytorch2.0_ttnn/issues?q=is%3Aissue%20state%3Aopen%20label%3Abounty) for some issues.

## Development Environment

To get started with development, you'll need a Wormhole or Blackhole Tenstorrent accelerator card, which:
* can be ordered on the [Tenstorrent website](https://tenstorrent.com/) 
* can be requested on [Koyeb](https://www.koyeb.com/blog/tenstorrent-cloud-instances-unveiling-next-gen-ai-accelerators)

Install the development dependencies and build the project (including the C++
extension) in editable mode from the tt-metal virtual environment created by
`create_venv.sh`:
```shell
pip install -e .[dev]
```

To rebuild the native extension after changing C++ sources, re-run the
installation command. The scikit-build-core backend will reuse the build
directory and pick up code changes automatically. See [docs/BuildFlow.md](docs/BuildFlow.md) for a
detailed walkthrough of the recommended workflow.

You can build a distributable wheel by running the modern PEP 517 build flow:
```shell
python -m build --wheel
```

**Note on TT_METAL_HOME**: If you have `TT_METAL_HOME` set in your environment (e.g., from working on tt-metal directly), the build system will detect it, display a warning, and **actively ignore** it. TT-Metal is always auto-detected from the git submodule at `torch_ttnn/cpp_extension/third-party/tt-metal`. This prevents build conflicts when switching between different TT projects (tt-metal, tt-train, pytorch2.0_ttnn).

## Project Structure

- `torch_ttnn/`: Main package directory containing the core implementation
- `tests/`: Test files for the project including model suites. We use `pytest` as our testing framework.
- `tools/`: Development and utility scripts
- `docs/`: Project documentation and reports
- `demo/`: Example code and usage demonstrations

## Questions and Support

If you have questions or need help getting started, please:
1. Review the existing documentation
2. Ask [PyTorch TT-NN DeepWiki](https://deepwiki.com/tenstorrent/pytorch2.0_ttnn) or [TT-Metal DeepWiki](https://deepwiki.com/tenstorrent/tt_metal)
3. Ask on [Discord](https://discord.gg/tenstorrent)
4. Open an issue on GitHub
