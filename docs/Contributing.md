# Operating Principles
Here are some things which we keep in mind day-to-day:
* We build trust showing where we are and making our pace apparent, no sugarcoating
* We expect every contribution to either a) inform us of a gap or b) improve the model pass rate
* We first clearly define a problem with a real-world based test that anyone can reproduce, then we solve it
* We encourage community contributions

In practice, we heavily rely on CI and the Model Test Suite to uphold these principles.

# What is Model's Lifecycle in the test suite?
Every model in the test suite goes through the following stages: <br>
`Traced` -> `Compiled` -> `E2E on device` -> `Performant`

### Traced
In this stage, a properly added model runs on the CPU to collect operation traces, which guide further development for both the compiler and TT-NN.
Examples: [Model trace](https://github.com/tenstorrent/pytorch2.0_ttnn/blob/main/docs/models/Bloom/input_variations.md) and [Operation trace](https://github.com/tenstorrent/pytorch2.0_ttnn/blob/main/docs/operations/aten._softmax.default.md).

### Compiled
At this stage, the compiler has successfully transformed PyTorch ATen operations into TT-NN operations, allowing the model to complete its run. 
Not all operations may be converted, and fallbacks can still be used if TT-NN or the compiler does not fully handle specific inputs.

### E2E on Device
This status requires that no fallbacks remain; every ATen call is fully replaced by a TT-NN call on the device.

### Performant
The performance target is not yet established. We will revisit this section once more than 50% of test models run E2E on the device.

# How to Add a New Model?
1. Create a new folder for the model [here](https://github.com/tenstorrent/pytorch2.0_ttnn/tree/main/tests/models).
2. Add a test file named `test_<modelname>.py`.
3. Follow [this example](https://github.com/tenstorrent/pytorch2.0_ttnn/blob/main/tests/models/bloom/test_bloom.py)
    * Define a class inheriting from `ModelTester` and implement the `_load_model` and `_load_inputs` methods.
    * Add parametrized pytest test function.
    * o enable testing for both training and inference, see this [example](https://github.com/tenstorrent/pytorch2.0_ttnn/blob/main/tests/models/mnist/test_mnist.py#L55)
4. Run the test locally or on CI (requires [Tenstorrent org](https://github.com/tenstorrent) membership).
5. If the model fails during compilation
    * Mark it with `@pytest.mark.compilation_xfail` so it can be merged as `Traced`.
    * Remember that making the model compile is your responsibility.

Lets grab some coffee once you add your first model! ☕️

# How to Enable Model Compilation?
To remove the `compilation_xfail` mark, follow these steps:

1. Update the autogenerated operation tests.
2. Run the tests.
3. Identify which operations from your model fail during isolated runs.
5. Follow the [example to enable fallback](https://github.com/tenstorrent/pytorch2.0_ttnn/pull/348/files) for these specific operations
6. Plan further work (by yourself or with tickets) to remove fallbacks to enable an E2E run

See this [troubleshooting guide](https://github.com/tenstorrent/pytorch2.0_ttnn/blob/main/docs/ProblemSolving.md) for tips on how to debug a failing model.

# How to Update the Autogenerated Model and Ops Report?
The below path is only available to members of [Tenstorrent org](https://github.com/tenstorrent).
1. Create a new branch ([you can change something in the README.md and create a PR from github UI](https://github.com/tenstorrent/pytorch2.0_ttnn/pull/352/commits/775a9051178f5f6e6b4e9bc0f3d33de0b0edb672))
2. Find the [workflow](https://github.com/tenstorrent/pytorch2.0_ttnn/actions/workflows/before_merge.yaml) 
3. Run the workflow on the created branch with `Docs` option
4. Open a PR, verify that the update looks good, and then merge.

![Screenshot 2024-10-29 at 7 04 15 PM](https://github.com/user-attachments/assets/284ce32b-75c9-4006-a22a-cd4725866330)

Want to improve the process? We aim to fully automate this to run nightly!

If you encounter issues, you can also run model tests locally and manually commit an updated report.

# How to Update Autogenerated Operation Tests?
Autogenerated Operation Tests gather input variations for each node from the model tests. This information is stored in the `metrics/` folder after running the model tests.

To update these tests, you can download the `metrics/` data from the CI:

1. Go to the CI page, scroll down, and download the `model-tests-metrics` artifacts.
2. Extract these artifacts into the `pytorch2.0_ttnn/metrics` directory.
3. Run the command: `python3 tools/generate_input_variation_test_from_models.py`.

The generated tests will be available in the `tests/autogen_op/` folder.