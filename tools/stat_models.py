import os
import argparse
import torch
import torchvision

from utils import get_model, model_example_inputs, do_model_backward


def get_ttnn_backend(
    model_name, trace_orig, trace_modi, backward, out_folder, graphviz, device
):
    use_tracer = trace_orig or trace_modi
    if use_tracer:
        tracer_option = {
            "model_name": model_name,
            "out_folder": out_folder,
            "trace_orig": trace_orig,
            "trace_modi": trace_modi,
        }
    else:
        tracer_option = None
    option = torch_ttnn.TorchTtnnOption(device=device, tracer_option=tracer_option)
    return torch.compile(m, backend="ttnn", options=option)


def get_dummy_backend(model_name, trace_orig, trace_modi, backward, out_folder):
    from torch._dynamo.backends.common import aot_autograd

    def dummy_backend(gm, example_inputs):
        return gm

    def dummy_backend_with_tracer(out_prefix):
        from tracer.tracer import Tracer

        return Tracer(
            dummy_backend,
            out_prefix=out_prefix,
            out_folder=out_folder,
            trace_orig=trace_orig,
            trace_modi=trace_modi,
        )

    use_tracer = trace_orig or trace_modi
    if not use_tracer and not backward:
        return aot_autograd(fw_compiler=dummy_backend)
    elif not use_tracer and backward:
        return aot_autograd(fw_compiler=dummy_backend, bw_compiler=dummy_backend)
    elif use_tracer and not backward:
        return aot_autograd(fw_compiler=dummy_backend_with_tracer(f"fw_{model_name}"))
    elif use_tracer and backward:
        return aot_autograd(
            fw_compiler=dummy_backend_with_tracer(f"fw_{model_name}"),
            bw_compiler=dummy_backend_with_tracer(f"bw_{model_name}"),
        )


def run_model(
    model_name: str,
    use_torch_ttnn: bool,
    trace_orig: bool,
    trace_modi,
    backward: bool,
    out_folder: str,
    graphviz: bool,
    to_profile: bool,
    device=None,
):
    m = get_model(model_name)
    if m is None:
        print(f"Skip this model: {model_name}")
        return

    if backward:
        try:
            m.train()
        except:
            print(f"{model_name} Cannot to the training mode, use just eval mode")
            m.eval()
            backward = False
    else:
        m.eval()

    if use_torch_ttnn:
        backend = get_ttnn_backend(
            model_name, trace_orig, trace_modi, backward, out_folder, graphviz, device
        )
    else:
        backend = get_dummy_backend(
            model_name, trace_orig, trace_modi, backward, out_folder
        )
    m = torch.compile(m, backend=backend)

    inputs = model_example_inputs(model_name, backward)

    if to_profile:
        from torch.profiler import profile, record_function, ProfilerActivity

        trace_file = os.path.join(out_folder, "profile", model_name)
        os.makedirs(os.path.dirname(trace_file), exist_ok=True)
        activities = [ProfilerActivity.CPU]
        if torch.cuda.is_available():
            activities.append(ProfilerActivity.CUDA)
        with profile(activities=activities, record_shapes=True) as prof:
            result = m(*inputs)
            if backward:
                do_model_backward(model_name, result)
        prof.export_chrome_trace(trace_file)
    else:
        result = m(*inputs)
        if backward:
            do_model_backward(model_name, result)


if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument(
        "--out_folder", "-o", type=str, default=os.path.join(os.getcwd(), "stat")
    )
    parser.add_argument("--use_torch_ttnn", action="store_true")
    parser.add_argument("--trace_orig", action="store_true")
    parser.add_argument("--trace_modi", action="store_true")
    parser.add_argument("--backward", action="store_true")
    parser.add_argument("--graphviz", action="store_true")
    parser.add_argument("--model_more", action="store_true")
    parser.add_argument("--model_sow2_list1", action="store_true")
    parser.add_argument("--profile", action="store_true")
    args = parser.parse_args()
    if args.use_torch_ttnn and args.backward:
        assert 0 and "torch_ttnn not yet support backward"
    if not args.use_torch_ttnn and args.graphviz:
        assert 0 and "graphviz is in ttnn"

    if args.use_torch_ttnn:
        import torch_ttnn

    models = ["dinov2_vits14", "alexnet", "googlenet", "resnet18", "vgg11"]
    if args.model_more:
        # There have some weired bug if squentially run the model seems because of torch.SymInt
        # torch._dynamo.exc.TorchRuntimeError:
        # Failed running call_function
        # <built-in method tensor of type object at 0x7f0ff3bff9e0>(*(0.00125*s9,), **{}):
        models = ["dinov2_vits14"] + torchvision.models.list_models()
    elif args.model_sow2_list1:
        models = [
            "detr_resnet50",
            "retinanet_resnet50_fpn",
            "mobilenet_v2",
            "mobilenet_v3_small",
            "vit_b_16",
            "resnet50",
            "regnet_x_16gf",
            "ssd300_vgg16",
            "ssdlite320_mobilenet_v3_large",
            "swin_v2_b",
        ]

    device = torch_ttnn.ttnn.open(0) if args.use_torch_ttnn else None
    for m in models:
        try:
            run_model(
                m,
                args.use_torch_ttnn,
                args.trace_orig,
                args.trace_modi,
                args.backward,
                args.out_folder,
                args.graphviz,
                args.profile,
                device,
            )
        except:
            print(f"{m} FAIL")
    if args.use_torch_ttnn:
        torch_ttnn.ttnn.close(device)
