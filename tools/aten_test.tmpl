import torch
import torch_ttnn
import pytest
import ttnn
from tests.utils import calculate_accuracy, get_input_vals_from_metric_str

class AtenModule(torch.nn.Module):
    def __init__(self):
        super().__init__()

    def forward(self, *args, **kwargs):
        return torch.ops.{opname}(*args, **kwargs)


@pytest.mark.parametrize(
    "input_strings",
    {inputs_strings}
)
def test_aten(device, input_strings, input_var_only_native, input_var_check_ttnn, input_var_check_accu):
    m = AtenModule()
    input_args, input_kwargs = get_input_vals_from_metric_str('{opname}', input_strings)
    if input_args is None and input_kwargs is None:
        pytest.skip("Invalid input strings:" + str(input_strings))
    result_before = m.forward(*input_args, **input_kwargs)
    if input_var_only_native:
        return
    option = torch_ttnn.TorchTtnnOption(device=device)
    #option.gen_graphviz = True
    # The compilation is lazy, so we need to run forward once to trigger the compilation
    m = torch.compile(m, backend=torch_ttnn.backend, options=option)
    result_after = m.forward(*input_args, **input_kwargs)
    #option._out_fx_graphs[0].print_tabular()

    if input_var_check_ttnn:
        # Check the graph has be rewritten and contain ttnn ops
        nodes = list(option._out_fx_graphs[0].nodes)
        assert any(["ttnn" in str(node) for node in nodes]), "No ttnn ops found in the graph"

    if input_var_check_accu:
        # Check inference result
        accuracy = calculate_accuracy(result_before, result_after)
        assert accuracy >= 0.99