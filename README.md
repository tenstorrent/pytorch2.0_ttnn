[comment]: <> (This README.md was generated by tools/collect_metrics.py.)
[comment]: <> (Please modify docs/README.md.in and/or collect_metrics.py to make permanent changes.)
[![Ask DeepWiki](https://deepwiki.com/badge.svg)](https://deepwiki.com/tenstorrent/pytorch2.0_ttnn)

# Visit [TT-Forge](https://github.com/tenstorrent/tt-forge) for our latest compiler project

**Pytorch 2.0 TT-NN** is no longer maintained, please consider using [TT-Forge](https://github.com/tenstorrent/tt-forge) instead.

The PyTorch 2.0 TT-NN Compiler enables seamless execution of PyTorch models on [Tenstorrent](https://tenstorrent.com/) AI accelerators. 
By leveraging the TT-NN backend, you can achieve significant performance improvements while maintaining PyTorch's familiar API.

## üöÄ Quick Start

### Installation

Install from the repo:
```bash
pip install git+https://bitbucket.org/tenstorrent/pytorch2.0_ttnn
```
or as an editable package from source:
```bash
git clone https://github.com/tenstorrent/pytorch2.0_ttnn.git
cd pytorch2.0_ttnn
pip install -e .
```

### ‚ú® Basic Usage

**Option 1: Eager Mode:** get your model running by switching to a TT device
```python
import torch
import torch_ttnn

model = YourModel()

device = ttnn.open_device(device_id=0)
model.to(torch_ttnn.ttnn_device_as_torch_device(device))

output = model(input_data)
```

**Option 2: Compilation Mode (Recommended):** get more perf with a JIT compiler
```python
import torch
import torch_ttnn

model = YourModel()

device = ttnn.open_mesh_device(ttnn.MeshShape(1, 2))  # 1x2 device grid
option = torch_ttnn.TorchTtnnOption(device=device, data_parallel=2)

model = torch.compile(model, backend=torch_ttnn.backend, options=option)
output = model(input_data)
```

## üìä Model Support

We've extensively tested the compiler across a diverse range of model architectures. Here's a summary of our validation results:

| Model                                                              | Status   |   Batch |   Compiled First Run (ms) |   Original Throughput (Inferences Per Second) |   Compiled Throughput (Inferences Per Second) |   Accuracy (%) | Torch Ops Before (Unique Ops)   | Torch Ops Remain (Unique Ops)   |   To/From Device Ops |
|:-------------------------------------------------------------------|:---------|--------:|--------------------------:|----------------------------------------------:|----------------------------------------------:|---------------:|:--------------------------------|:--------------------------------|---------------------:|
| [Mistral-7B-Instruct-v0.3](<docs/models/Mistral-7B-Instruct-v0.3>) | üöß       |      32 |                   42481.1 |                                      0.687465 |                                      0.753277 |          99.61 | 3629 (29)                       | 2 (2)                           |                  160 |

### Explanation of Metrics

**Model**: Name of the model.  
**Status**: Indicates whether the model is:
- ‚úÖ End-to-end on device: All PyTorch operations have been converted to TT-NN operations.
- üöß Compiled: The converted model runs but some operations still fallback to PyTorch. This may be due to an unsupported operation or configuration.
- ‚ùå Traced: The model does not run but its PyTorch operations are traced for future development. This may indicate a temporary incompatibility with a compiler pass.  
**Batch**: Batch size used for inference  
**Compiled First Run (ms)**: Time until the first compiled run finishes (ms), including compilation time and warming caches.  
**Original Throughput (Inferences Per Second)**: Execution throughput (in inferences per second) of the model before conversion.  
**Compiled Throughput (Inferences Per Second)**: Execution throughput (in inferences per second) of the model after conversion, once caches are warm.  
**Accuracy (%)**: Model accuracy on a predefined test dataset after conversion.  
**Torch Ops Before (Unique Ops)**: The total number of operations used by the model in the original Torch implementation. The number in parentheses represents the total unique ops.  
**Torch Ops Remain (Unique Ops)**: The total number of operations used after conversion to TT-NN. The number in parentheses represents the total unique ops.  
**To/From Device Ops**: The number of `to/from_device` operations (data transfer to/from the device).  

***

# Contributing

Whether you are new to Tenstorrent hardware or an experienced developer, there are many ways to contribute.

## Getting Started

Start with our high level [Contribution guide](docs/Contributing.md).
You can find more information here:
* [Discussions](https://github.com/tenstorrent/pytorch2.0_ttnn/discussions)
* [Operations Report](docs/OperationsReport.md)
* [Lowering TT-NN Operation to PyTorch](docs/AddNewOperationLowering.md)
* [Native Device Integration Extension](docs/OpenRegistrationAPI.md)
* [Build with Metal from Source](docs/DevelopWithMetalFromSources.md)
* [Known Issues](docs/KnownIssues.md)
* [Problem Solving](docs/ProblemSolving.md)

We encourage contributions and offer [ü§ë Bounties](https://github.com/tenstorrent/pytorch2.0_ttnn/issues?q=is%3Aissue%20state%3Aopen%20label%3Abounty) for some issues.

## Development Environment

To get started with development, you'll need a Wormhole or Blackhole Tenstorrent accelerator card, which:
* can be ordered on the [Tenstorrent website](https://tenstorrent.com/) 
* can be requested on [Koyeb](https://www.koyeb.com/blog/tenstorrent-cloud-instances-unveiling-next-gen-ai-accelerators)

Install the development dependencies:
```shell
pip install -r requirements-dev.txt
pip install -e .
```

You can build the wheel file with
```shell
python -m build
```

## Project Structure

- `torch_ttnn/`: Main package directory containing the core implementation
- `tests/`: Test files for the project including model suites. We use `pytest` as our testing framework.
- `tools/`: Development and utility scripts
- `docs/`: Project documentation and reports
- `demo/`: Example code and usage demonstrations

## Questions and Support

If you have questions or need help getting started, please:
1. Review the existing documentation
2. Ask [PyTorch TT-NN DeepWiki](https://deepwiki.com/tenstorrent/pytorch2.0_ttnn) or [TT-Metal DeepWiki](https://deepwiki.com/tenstorrent/tt_metal)
3. Ask on [Discord](https://discord.gg/tenstorrent)
4. Open an issue on GitHub

