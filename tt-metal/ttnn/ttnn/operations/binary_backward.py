# SPDX-FileCopyrightText: Â© 2023 Tenstorrent Inc.

# SPDX-License-Identifier: Apache-2.0

import sys

import ttnn
from ttnn.operations.complex_binary_backward import (
    _golden_function_complex_add,
    _golden_function_complex_sub,
    _golden_function_complex_mul,
    _golden_function_complex_div,
)


THIS_MODULE = sys.modules[__name__]

__all__ = []


def _golden_function_backward(torch_op, grad_tensor, input_tensor_a, input_tensor_b, *args, **kwargs):
    import torch

    if torch.is_complex(input_tensor_a):
        if torch_op == torch.add:
            alpha = kwargs.pop("alpha")
            return _golden_function_complex_add(grad_tensor, input_tensor_a, input_tensor_b, alpha)
        elif torch_op == torch.sub:
            alpha = kwargs.pop("alpha")
            return _golden_function_complex_sub(grad_tensor, input_tensor_a, input_tensor_b, alpha)
        elif torch_op == torch.mul:
            return _golden_function_complex_mul(grad_tensor, input_tensor_a, input_tensor_b)
    elif torch_op == torch.add or torch_op == torch.sub or torch_op == torch.mul:
        return _golden_function_backward_overload(torch_op, grad_tensor, input_tensor_a, input_tensor_b)
    if torch_op == "torch.squared_difference":
        pyt_y = torch.square(torch.sub(input_tensor_a, input_tensor_b))
    else:
        pyt_y = torch_op(input_tensor_a, input_tensor_b)
    input_tensor_a.retain_grad()
    input_tensor_b.retain_grad()
    pyt_y.backward(gradient=grad_tensor)
    golden_tensor = [input_tensor_a.grad, input_tensor_b.grad]
    return golden_tensor


def _golden_function_backward_overload(torch_op, grad_tensor, input_tensor_a, input_tensor_b=None, *args, **kwargs):
    import torch

    if torch_op == torch.clone:
        pyt_y = torch.clone(input_tensor_a)
        input_tensor_a.retain_grad()
        pyt_y.backward(gradient=grad_tensor)
        if input_tensor_b == None:
            golden_tensor = [input_tensor_a.grad]
            return golden_tensor
        else:
            golden_tensor = [input_tensor_a.grad, input_tensor_a.grad]
            return golden_tensor
    pyt_y = torch_op(input_tensor_a, input_tensor_b)
    if isinstance(input_tensor_b, (float, int)):
        input_tensor_a.retain_grad()
        pyt_y.backward(gradient=grad_tensor)
        golden_tensor = [input_tensor_a.grad]
        return golden_tensor
    input_tensor_a.retain_grad()
    input_tensor_b.retain_grad()
    pyt_y.backward(gradient=grad_tensor)
    golden_tensor = [input_tensor_a.grad, input_tensor_b.grad]
    return golden_tensor


def _golden_function_backward_with_dim(
    torch_op, grad_tensor, input_tensor_a, input_tensor_b, dimension=None, *args, **kwargs
):
    import torch

    if input_tensor_a.requires_grad is False:
        input_tensor_a.requires_grad = True
    if input_tensor_b.requires_grad is False:
        input_tensor_b.requires_grad = True

    input_tensor_a.retain_grad()
    input_tensor_b.retain_grad()

    if dimension == None:
        pyt_y = torch.concat((input_tensor_a, input_tensor_b))
    else:
        pyt_y = torch.concat((input_tensor_a, input_tensor_b), dim=dimension)
    pyt_y.backward(gradient=grad_tensor)
    golden_tensor = [input_tensor_a.grad, input_tensor_b.grad]
    return golden_tensor


def _golden_function_backward_with_float(
    torch_op, grad_tensor, input_tensor_a, input_tensor_b, alpha=None, *args, **kwargs
):
    import torch

    if alpha == None:
        pyt_y = torch_op(input_tensor_a, input_tensor_b)
    else:
        pyt_y = torch_op(input_tensor_a, input_tensor_b, alpha=alpha)
    input_tensor_a.retain_grad()
    input_tensor_b.retain_grad()
    pyt_y.backward(gradient=grad_tensor)
    golden_tensor = [input_tensor_a.grad, input_tensor_b.grad]
    return golden_tensor


def _golden_function_backward_with_string(
    torch_op, grad_tensor, input_tensor_a, input_tensor_b, value=None, *args, **kwargs
):
    import torch

    if torch.is_complex(input_tensor_a):
        if torch_op == torch.div:
            return _golden_function_complex_div(grad_tensor, input_tensor_a, input_tensor_b)
    if torch_op == "bias_gelu_bw":
        sum_result = torch.add(input_tensor_a, input_tensor_b)
        pyt_y = torch.nn.functional.gelu(sum_result, approximate=value)
        sum_result.retain_grad()
        pyt_y.backward(gradient=grad_tensor)
        if isinstance(input_tensor_b, (float, int)):
            golden_tensor = [sum_result.grad]
        else:
            golden_tensor = [sum_result.grad, sum_result.grad]
        return golden_tensor
    elif torch_op == torch.div:
        pyt_y = torch_op(input_tensor_a, input_tensor_b, rounding_mode=value)
    else:
        pyt_y = torch_op(input_tensor_a, input_tensor_b, value=value)
    if isinstance(input_tensor_b, (float, int)):
        input_tensor_a.retain_grad()
        pyt_y.backward(gradient=grad_tensor)
        golden_tensor = [input_tensor_a.grad]
        return golden_tensor
    input_tensor_a.retain_grad()
    input_tensor_b.retain_grad()
    pyt_y.backward(gradient=grad_tensor)
    golden_tensor = [input_tensor_a.grad, input_tensor_b.grad]
    return golden_tensor


def _golden_function_bw(grad, a, b, *args, **kwargs):
    import torch

    return _golden_function_backward(torch.sub, grad, a, b, *args, **kwargs)


ttnn.attach_golden_function(ttnn.sub_bw, golden_function=_golden_function_bw)


def _golden_function_bw(grad, a, b, *args, **kwargs):
    import torch

    return _golden_function_backward(torch.add, grad, a, b, *args, **kwargs)


ttnn.attach_golden_function(ttnn.add_bw, golden_function=_golden_function_bw)


def _golden_function_bw(grad, a, b, *args, **kwargs):
    import torch

    return _golden_function_backward_overload(torch.remainder, grad, a, b, *args, **kwargs)


ttnn.attach_golden_function(ttnn.remainder_bw, golden_function=_golden_function_bw)


def _golden_function_bw(grad, a, b, *args, **kwargs):
    import torch

    return _golden_function_backward_overload(torch.fmod, grad, a, b, *args, **kwargs)


ttnn.attach_golden_function(ttnn.fmod_bw, golden_function=_golden_function_bw)


def _golden_function_bw(grad, a, b, *args, **kwargs):
    import torch

    return _golden_function_backward(torch.atan2, grad, a, b, *args, **kwargs)


ttnn.attach_golden_function(ttnn.atan2_bw, golden_function=_golden_function_bw)


def _golden_function_bw(grad, a, b, *args, **kwargs):
    import torch

    return _golden_function_backward(torch.xlogy, grad, a, b, *args, **kwargs)


ttnn.attach_golden_function(ttnn.xlogy_bw, golden_function=_golden_function_bw)


def _golden_function_bw(grad, a, b, *args, **kwargs):
    import torch

    return _golden_function_backward(torch.hypot, grad, a, b, *args, **kwargs)


ttnn.attach_golden_function(ttnn.hypot_bw, golden_function=_golden_function_bw)


def _golden_function_bw(grad, a, b, *args, **kwargs):
    import torch

    return _golden_function_backward(torch.ldexp, grad, a, b, *args, **kwargs)


ttnn.attach_golden_function(ttnn.ldexp_bw, golden_function=_golden_function_bw)


def _golden_function_bw(grad, a, b, *args, **kwargs):
    import torch

    return _golden_function_backward(torch.logaddexp, grad, a, b, *args, **kwargs)


ttnn.attach_golden_function(ttnn.logaddexp_bw, golden_function=_golden_function_bw)


def _golden_function_bw(grad, a, b, *args, **kwargs):
    import torch

    return _golden_function_backward(torch.logaddexp2, grad, a, b, *args, **kwargs)


ttnn.attach_golden_function(ttnn.logaddexp2_bw, golden_function=_golden_function_bw)


def _golden_function_bw(grad, a, b, *args, **kwargs):
    return _golden_function_backward("torch.squared_difference", grad, a, b, *args, **kwargs)


ttnn.attach_golden_function(ttnn.squared_difference_bw, golden_function=_golden_function_bw)


def _golden_function_bw(grad, a, b, alpha=None, *args, **kwargs):
    import torch

    return _golden_function_backward_with_float(torch.sub, grad, a, b, alpha, *args, **kwargs)


ttnn.attach_golden_function(ttnn.subalpha_bw, golden_function=_golden_function_bw)


def _golden_function_bw(grad, a, b, alpha=None, *args, **kwargs):
    import torch

    return _golden_function_backward_with_float(torch.add, grad, a, b, alpha, *args, **kwargs)


ttnn.attach_golden_function(ttnn.addalpha_bw, golden_function=_golden_function_bw)


def _golden_function_bw(grad, a, b=None, *args, **kwargs):
    import torch

    return _golden_function_backward_overload(torch.clone, grad, a, b, *args, **kwargs)


ttnn.attach_golden_function(ttnn.assign_bw, golden_function=_golden_function_bw)


def _golden_function_bw(grad, a, b, dim=None, *args, **kwargs):
    import torch

    return _golden_function_backward_with_dim(torch.concat, grad, a, b, dim, *args, **kwargs)


ttnn.attach_golden_function(ttnn.concat_bw, golden_function=_golden_function_bw)


def _golden_function_bw(grad, a, b, *args, **kwargs):
    import torch

    return _golden_function_backward(torch.rsub, grad, a, b, *args, **kwargs)


ttnn.attach_golden_function(ttnn.rsub_bw, golden_function=_golden_function_bw)


def _golden_function_bw(grad, a, b, value="none", *args, **kwargs):
    import torch

    return _golden_function_backward_with_string("bias_gelu_bw", grad, a, b, value, *args, **kwargs)


ttnn.attach_golden_function(ttnn.bias_gelu_bw, golden_function=_golden_function_bw)


def _golden_function_bw(grad, a, b, *args, **kwargs):
    import torch

    return _golden_function_backward(torch.min, grad, a, b, *args, **kwargs)


ttnn.attach_golden_function(ttnn.min_bw, golden_function=_golden_function_bw)


def _golden_function(grad, a, b, *args, **kwargs):
    import torch

    return _golden_function_backward(torch.max, grad, a, b, *args, **kwargs)


ttnn.attach_golden_function(ttnn.max_bw, golden_function=_golden_function)


def _golden_function(grad, a, b, value=None, *args, **kwargs):
    import torch

    return _golden_function_backward_with_string(torch.div, grad, a, b, value, *args, **kwargs)


ttnn.attach_golden_function(ttnn.div_bw, golden_function=_golden_function)


def _golden_function(grad, a, b, *args, **kwargs):
    import torch

    return _golden_function_backward(torch.mul, grad, a, b, *args, **kwargs)


ttnn.attach_golden_function(ttnn.mul_bw, golden_function=_golden_function)


__all__ = []
