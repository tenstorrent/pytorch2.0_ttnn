
// SPDX-FileCopyrightText: Â© 2023 Tenstorrent Inc.
//
// SPDX-License-Identifier: Apache-2.0

#include "attention_softmax.hpp"

#include "ttnn/operations/eltwise/binary/binary.hpp"

namespace ttnn::operations::transformer {

template <bool in_place>
ttnn::Tensor ExecuteAttentionSoftmax<in_place>::invoke(
    const ttnn::Tensor& input_tensor,
    const std::optional<int>& head_size_arg,
    const std::optional<const ttnn::Tensor>& attention_mask,
    const ttnn::operations::normalization::SoftmaxProgramConfig& program_config,
    const std::optional<bool> causal_mask,
    const std::optional<ttnn::MemoryConfig>& memory_config) {
    float head_size = head_size_arg.has_value() ? 1.0f / std::sqrt(head_size_arg.value()) : 1.0f;
    if constexpr (in_place) {
        TT_FATAL(
            attention_mask.has_value(),
            "Cannot apply divide by sqrt(head_size) using in-place version when attention_mask is not set.");
    } else {
        if (not attention_mask.has_value()) {
            auto output_tensor = ttnn::multiply(input_tensor, head_size);
            return ttnn::operations::normalization::softmax(
                output_tensor, memory_config.value_or(input_tensor.memory_config()));
        }
    }

    std::optional<const DeviceComputeKernelConfig> compute_kernel_config = std::nullopt;
    auto kernel_config_val = init_device_compute_kernel_config(
        input_tensor.device()->arch(), compute_kernel_config, MathFidelity::HiFi4, true, false, false);
    auto output_tensor = tt::tt_metal::operation::run(
                             ttnn::operations::normalization::Softmax{
                                 head_size,
                                 in_place,
                                 memory_config.value_or(input_tensor.memory_config()),
                                 program_config,
                                 causal_mask.value(),
                                 kernel_config_val,
                                 false},
                             {input_tensor},
                             {attention_mask})
                             .at(0);
    return output_tensor;
}

template struct ExecuteAttentionSoftmax<false>;
template struct ExecuteAttentionSoftmax<true>;

}  // namespace ttnn::operations::transformer
