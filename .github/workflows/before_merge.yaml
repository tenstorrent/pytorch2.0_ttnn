name: "Tests"

on:
  workflow_call:
  workflow_dispatch:
    inputs:
      commit_report:
        description: 'Commit generated report files: None, Readme, All'
        required: false
        default: 'None'
  merge_group:

permissions:
  actions: read
  contents: write
  pages: write
  id-token: write
  pull-requests: write

jobs:
  validate-pr:
    env:
      ARCH_NAME: wormhole_b0
    runs-on: ["in-service", "n150"]
    steps:
      - name: Collect Workflow Telemetry
        uses: catchpoint/workflow-telemetry-action@v2        
      - name: Checkout Repo
        uses: actions/checkout@v4
      - name: Install dependencies
        run: |
          python3 -m venv venv
          source venv/bin/activate
          python3 -m pip config set global.extra-index-url https://download.pytorch.org/whl/cpu
          python3 -m pip install --upgrade pip                    
          python3 -m pip install -r requirements-dev.txt
          python3 -m pip install pytest-github-report
      - name: Run Tools Tests
        env:
          pytest_verbosity: 2
          pytest_report_title: "⭐️ Tools Tests"
        run: | 
          source venv/bin/activate
          python3 -m pytest --github-report tests/tools/ -s      
      - name: Run Lowering Tests
        id: lowering_tests
        env:
          pytest_verbosity: 2
          pytest_report_title: "⭐️ Aten → TTNN Lowering Tests"
        run: | 
          source venv/bin/activate
          python3 -m pytest --github-report tests/lowering/ -s
      - name: Check runner debug
        if: ${{ runner.debug == '1' }}
        run: |
          echo "TT_METAL_WATCHER=120" >> $GITHUB_ENV
          echo "TT_METAL_WATCHER_APPEND=0" >> $GITHUB_ENV
          echo "TT_METAL_WATCHER_DUMP_ALL=1" >> $GITHUB_ENV
      - name: Collect Model Tests
        id: collect-tests
        run: |
          tests=$(pytest --collect-only -q tests/models | grep '::' | awk '{print $1}')
          echo "$tests" > tests.txt
      - name: Run Model Tests
        id: model_tests
        if: steps.lowering_tests.outcome == 'success'
        env:
          pytest_verbosity: 2
          pytest_report_title: "⭐️ Model Tests"
        run: | 
          source venv/bin/activate          
          while IFS= read -r test; do
            echo "::group::Running test: $test"
            python3 -m pytest --github-report "$test" -s || echo echo "Test $test failed"
            echo "::endgroup::"
          done < tests.txt
      - name: Collect Metrics Report
        if: ${{ github.event_name == 'workflow_dispatch' && steps.lowering_tests.outcome == 'success' && steps.model_tests.outcome == 'success' }}
        env: 
          PYTHONPATH: ${{ github.workspace }}
        run: | 
          source venv/bin/activate          
          python3 tools/collect_metrics.py
          git config user.name "GitHub Actions"
          git config user.email "actions@github.com"          
          git status
          
          if [ "${{ github.event.inputs.commit_option }}" == "All" ]; then
            git add .
          elif [ "${{ github.event.inputs.commit_option }}" == "Readme" ]; then
            git add README.md
          elif [ "${{ github.event.inputs.commit_option }}" == "None" ]; then
            echo "No files will be committed"
            exit 0
          fi
          
          if git diff-index --quiet HEAD; then
            echo "No changes to commit"
          else
            git commit -m '[auto][on-merge-queue] Update metrics report in README.md'
            git push
          fi
