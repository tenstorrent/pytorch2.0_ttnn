name: "Tests"

on:
  workflow_call:
  workflow_dispatch:
    inputs:
      commit_report:
        description: 'Commit generated report files: None, Readme, All'
        required: false
        default: 'None'
  merge_group:

permissions:
  actions: read
  contents: write
  pages: write
  id-token: write
  pull-requests: write

jobs:
  validate-pr:
    env:
      ARCH_NAME: wormhole_b0
    runs-on: ["in-service", "n150"]
    steps:
      - name: Collect Workflow Telemetry
        uses: catchpoint/workflow-telemetry-action@v2
        
      - name: Checkout Repo
        uses: actions/checkout@v4
        
      - name: Install dependencies
        run: |
          python3 -m venv venv
          source venv/bin/activate
          python3 -m pip config set global.extra-index-url https://download.pytorch.org/whl/cpu
          python3 -m pip install --upgrade pip                    
          python3 -m pip install -r requirements-dev.txt
          python3 -m pip install pytest-github-report
          
      - name: Run Tools Tests
        env:
          pytest_verbosity: 2
          pytest_report_title: "⭐️ Tools Tests"
        run: | 
          source venv/bin/activate
          python3 -m pytest --github-report tests/tools/ -s
          
      - name: Run Lowering Tests
        id: lowering_tests
        env:
          pytest_verbosity: 2
          pytest_report_title: "⭐️ Aten → TTNN Lowering Tests"
        run: | 
          source venv/bin/activate
          python3 -m pytest --github-report tests/lowering/ -s
          
      - name: Check runner debug
        if: ${{ runner.debug == '1' }}
        run: |
          echo "TT_METAL_WATCHER=120" >> $GITHUB_ENV
          echo "TT_METAL_WATCHER_APPEND=0" >> $GITHUB_ENV
          echo "TT_METAL_WATCHER_DUMP_ALL=1" >> $GITHUB_ENV

      # Run each group of model tests in separate steps using pytest-split
      # Doing this manually like this to check if memory usage will normalize
      - name: Run Model Tests Group 1
        if: steps.lowering_tests.outcome == 'success'
        env:
          pytest_verbosity: 2
          pytest_report_title: "⭐️ Model Tests - Group 1"
        run: |
          source venv/bin/activate
          python3 -m pytest --github-report tests/models/ --split 10 --group 1 -s
          huggingface-cli delete-cache
          rm -rf ~/.torch/models
          free -h

      - name: Run Model Tests Group 2
        if: steps.lowering_tests.outcome == 'success'
        env:
          pytest_verbosity: 2
          pytest_report_title: "⭐️ Model Tests - Group 2"
        run: |
          source venv/bin/activate
          python3 -m pytest --github-report tests/models/ --split 10 --group 2 -s
          huggingface-cli delete-cache
          rm -rf ~/.torch/models
          free -h

      - name: Run Model Tests Group 3
        if: steps.lowering_tests.outcome == 'success'
        env:
          pytest_verbosity: 2
          pytest_report_title: "⭐️ Model Tests - Group 3"
        run: |
          source venv/bin/activate
          python3 -m pytest --github-report tests/models/ --split 10 --group 3 -s
          huggingface-cli delete-cache
          rm -rf ~/.torch/models
          free -h

      - name: Run Model Tests Group 4
        if: steps.lowering_tests.outcome == 'success'
        env:
          pytest_verbosity: 2
          pytest_report_title: "⭐️ Model Tests - Group 4"
        run: |
          source venv/bin/activate
          python3 -m pytest --github-report tests/models/ --split 10 --group 4 -s
          huggingface-cli delete-cache
          rm -rf ~/.torch/models
          free -h

      - name: Run Model Tests Group 5
        if: steps.lowering_tests.outcome == 'success'
        env:
          pytest_verbosity: 2
          pytest_report_title: "⭐️ Model Tests - Group 5"
        run: |
          source venv/bin/activate
          python3 -m pytest --github-report tests/models/ --split 10 --group 5 -s
          huggingface-cli delete-cache
          rm -rf ~/.torch/models
          free -h

      - name: Run Model Tests Group 6
        if: steps.lowering_tests.outcome == 'success'
        env:
          pytest_verbosity: 2
          pytest_report_title: "⭐️ Model Tests - Group 6"
        run: |
          source venv/bin/activate
          python3 -m pytest --github-report tests/models/ --split 10 --group 6 -s
          huggingface-cli delete-cache
          rm -rf ~/.torch/models
          free -h

      - name: Run Model Tests Group 7
        if: steps.lowering_tests.outcome == 'success'
        env:
          pytest_verbosity: 2
          pytest_report_title: "⭐️ Model Tests - Group 7"
        run: |
          source venv/bin/activate
          python3 -m pytest --github-report tests/models/ --split 10 --group 7 -s
          huggingface-cli delete-cache
          rm -rf ~/.torch/models
          free -h

      - name: Run Model Tests Group 8
        if: steps.lowering_tests.outcome == 'success'
        env:
          pytest_verbosity: 2
          pytest_report_title: "⭐️ Model Tests - Group 8"
        run: |
          source venv/bin/activate
          python3 -m pytest --github-report tests/models/ --split 10 --group 8 -s
          huggingface-cli delete-cache
          rm -rf ~/.torch/models
          free -h

      - name: Run Model Tests Group 9
        if: steps.lowering_tests.outcome == 'success'
        env:
          pytest_verbosity: 2
          pytest_report_title: "⭐️ Model Tests - Group 9"
        run: |
          source venv/bin/activate
          python3 -m pytest --github-report tests/models/ --split 10 --group 9 -s
          huggingface-cli delete-cache
          rm -rf ~/.torch/models
          free -h

      - name: Run Model Tests Group 10
        if: steps.lowering_tests.outcome == 'success'
        env:
          pytest_verbosity: 2
          pytest_report_title: "⭐️ Model Tests - Group 10"
        run: |
          source venv/bin/activate
          python3 -m pytest --github-report tests/models/ --split 10 --group 10 -s
          huggingface-cli delete-cache
          rm -rf ~/.torch/models
          free -h

      # Collect Metrics
      - name: Collect Metrics Report
        if: ${{ github.event_name == 'workflow_dispatch' && steps.lowering_tests.outcome == 'success' && steps.model_tests.outcome == 'success' }}
        env: 
          PYTHONPATH: ${{ github.workspace }}
        run: | 
          source venv/bin/activate          
          python3 tools/collect_metrics.py
          git config user.name "GitHub Actions"
          git config user.email "actions@github.com"          
          git status
          
          if [ "${{ github.event.inputs.commit_option }}" == "All" ]; then
            git add .
          elif [ "${{ github.event.inputs.commit_option }}" == "Readme" ]; then
            git add README.md
          elif [ "${{ github.event.inputs.commit_option }}" == "None" ]; then
            echo "No files will be committed"
            exit 0
          fi
          
          if git diff-index --quiet HEAD; then
            echo "No changes to commit"
          else
            git commit -m '[auto][on-merge-queue] Update metrics report in README.md'
            git push
          fi
