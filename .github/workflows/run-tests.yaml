name: "Run Tests"
on:
  workflow_call:
    inputs:
      commit_report:
        description: 'Commit generated report files: None, Docs, All'
        required: false
        type: string
        default: 'None'
      docker_tag:
        description: 'Docker container tag to use'
        required: false
        type: string
        default: 'ghcr.io/tenstorrent/pytorch2.0_ttnn/ubuntu-22.04-amd64:latest'
    outputs:
      tests_passed:
        description: "Whether tests passed (0 means pass, 1 means fail)"
        value: ${{ jobs.tests-passed.outputs.didpass }}
  workflow_dispatch:
    inputs:
      commit_report:
        description: 'Commit generated report files: None, Docs, All'
        required: false
        type: string
        default: 'None'
      docker_tag:
        description: 'Docker container tag to use'
        required: false
        type: string
        default: 'ghcr.io/tenstorrent/pytorch2.0_ttnn/ubuntu-22.04-amd64:latest'

permissions:
  actions: read
  contents: write
  pages: write
  id-token: write
  pull-requests: write

jobs:
  tools-tests:
    env:
      pytest_verbosity: 2    
      pytest_report_title: "⭐️ Tools Tests"
    runs-on: ["in-service"]
    container: 
      image: ${{ inputs.docker_tag }}
      credentials:
        username: ${{ github.actor }}
        password: ${{ secrets.GH_TOKEN }}
      options: >-
        --rm -v /dev/hugepages-1G:/dev/hugepages-1G --device /dev/tenstorrent
        -v ${{ github.workspace }}:${{ github.workspace }} -w ${{ github.workspace }}
    steps:      
      - uses: actions/checkout@v4
      - uses: ./.github/actions/common_repo_setup
      - name: Run Tools Tests 
        run: |
          python3 -m pytest --github-report tests/tools/ -s

  lowering-tests:
    runs-on: ["in-service"]
    container: 
      image: ${{ inputs.docker_tag }}
      credentials:
        username: ${{ github.actor }}
        password: ${{ secrets.GH_TOKEN }}
      options: >-
        --rm -v /dev/hugepages-1G:/dev/hugepages-1G --device /dev/tenstorrent
        -v ${{ github.workspace }}:${{ github.workspace }} -w ${{ github.workspace }}
    env:
      pytest_verbosity: 2    
      pytest_report_title: "⭐️ Aten → TTNN Lowering Tests"
    strategy:
      matrix: # Need to find a way to replace this with a generator
        group: [1, 2]
    steps:
      - uses: actions/checkout@v4
      - uses: ./.github/actions/common_repo_setup
      - uses: ./.github/actions/common_lowering_tests

  count-test-files:
    runs-on: ubuntu-latest
    outputs:
      matrix: ${{ steps.count-files.outputs.matrix }}
      num_files: ${{ steps.count-files.outputs.num_files }}
    steps:
      - name: Checkout Code
        uses: actions/checkout@v4

      - name: Count Test Files
        id: count-files
        uses: ./.github/actions/count_test_files
        with:
          test_directory: 'tests/models/'

  model-tests:
    needs: [count-test-files, lowering-tests]
    runs-on: ["in-service", "nfs"]
    container: 
      image: ${{ inputs.docker_tag }}
      credentials:
        username: ${{ github.actor }}
        password: ${{ secrets.GH_TOKEN }}
      options: >-
        --rm -v /dev/hugepages-1G:/dev/hugepages-1G --device /dev/tenstorrent
        -v ${{ github.workspace }}:${{ github.workspace }} -w ${{ github.workspace }}
        -v /mnt/tt-metal-pytorch-cache/.cache:/root/.cache
    env:      
      pytest_verbosity: 0
      pytest_report_title: "⭐️ Model Tests - Group ${{ matrix.group }}"    
      TORCH_HOME: /mnt/tt-metal-pytorch-cache/.cache/torch
      HF_HOME: /mnt/tt-metal-pytorch-cache/.cache/huggingface
    strategy:
      matrix: 
        group: ${{ fromJson(needs.count-test-files.outputs.matrix) }}
    steps:
      - uses: actions/checkout@v4
        with:
          lfs: true
          fetch-depth: 0    
      - name: docker-cleanup
        run: |
          docker system prune -a -f --volumes
          df -h  # Debug space
      - uses: ./.github/actions/common_model_tests
        with:
          splits: ${{ needs.count-test-files.outputs.num_files }}
          matrix_group: ${{ matrix.group }}
          commit_report: ${{ github.event.inputs.commit_report }}
      - name: Upload Metrics Artifact
        if: success()  # Only run if tests passed
        uses: actions/upload-artifact@v4
        with:
          name: model-tests-metrics-group-${{ matrix.group }}
          path: metrics/

  push-autogen-op-tests:
    needs: [model-tests]
    if: ${{ github.event_name == 'workflow_dispatch' && inputs.commit_report != 'None'}}
    runs-on: ["in-service"]
    container: 
      image: ${{ inputs.docker_tag }}
      credentials:
        username: ${{ github.actor }}
        password: ${{ secrets.GH_TOKEN }}
      options: >-
        --rm -v /dev/hugepages-1G:/dev/hugepages-1G --device /dev/tenstorrent
        -v ${{ github.workspace }}:${{ github.workspace }} -w ${{ github.workspace }}
    env:
      PYTHONPATH: ${{ github.workspace }}
    steps:
      - uses: actions/checkout@v4
      - uses: ./.github/actions/common_repo_setup

      - name: Download All Metrics Artifacts
        uses: actions/download-artifact@v4
        with:
          pattern: model-tests-metrics-group-*
          merge-multiple: true
          path: metrics/

      - name: Push Autogen Op Tests
        run: |
          git config user.name "GitHub Actions"
          git config user.email "actions@github.com"
          git pull

          rm -rf tests/autogen_op/ # Remove old tests
          python3 tools/generate_input_variation_test_from_models.py
          python3 tools/generate_input_variation_test_from_models.py --merge=True
          if [ "${{ inputs.commit_report }}" == "All" ]; then
            git add --all
          elif [ "${{ inputs.commit_report }}" == "Tests" ]; then
            git add tests/
          else
            echo "No files will be committed"
            exit 0
          fi
          if git diff --cached --quiet; then
            echo "No changes to commit"
            exit 0
          else
            git commit -m '[auto][on-merge-queue] Update input variations tests'
            git push
          fi

  model-autogen-op-tests:
    needs: [push-autogen-op-tests, count-test-files]
    if: ${{ github.event_name == 'workflow_dispatch' && inputs.commit_report != 'None'}}
    runs-on: ["in-service"]
    container: 
      image: ${{ inputs.docker_tag }}
      credentials:
        username: ${{ github.actor }}
        password: ${{ secrets.GH_TOKEN }}
      options: >-
        --rm -v /dev/hugepages-1G:/dev/hugepages-1G --device /dev/tenstorrent
        -v ${{ github.workspace }}:${{ github.workspace }} -w ${{ github.workspace }}
    strategy:
      matrix: 
        group: ${{ fromJson(needs.count-test-files.outputs.matrix) }}
    env:
      pytest_verbosity: 0
      pytest_report_title: "⭐️ Model Input Variations Tests - Group ${{ matrix.group }}"
    steps:
      - uses: actions/checkout@v4
      - uses: ./.github/actions/common_repo_setup

      - name: Run Model Input Variations Tests
        run: |
          set +e
          git pull
          rm -rf tests/autogen_op/ALL
          python3 -m pytest --github-report tests/autogen_op --splits 40 --group ${{ matrix.group }} -s
          exit_code=$?  # Capture the exit code, but ignore any errors for now.
          ls -l
          cd metrics-autogen-op
          ls -l
          cd ..
          exit 0;

      - name: Upload Input Variations Metrics Artifact
        if: success()  # Only run if tests passed
        uses: actions/upload-artifact@v4
        with:
          name: model-autogen-op-tests-metrics-group-${{ matrix.group }}
          path: metrics-autogen-op/

  collect-model-artifacts-from-matrix-jobs:
    needs: model-tests
    runs-on: ubuntu-latest
    env:
      pytest_verbosity: 0
    steps:
      - uses: actions/checkout@v4
      - uses: ./.github/actions/common_repo_setup
      - name: Download All Metrics Artifacts
        uses: actions/download-artifact@v4
        with:
          pattern: model-tests-metrics-group-*
          merge-multiple: true
          path: metrics/

      - name: Upload Metrics Artifact
        if: success()  # Only run if tests passed
        uses: actions/upload-artifact@v4
        with:
          name: model-tests-metrics
          path: metrics/

  collect-op-artifacts-from-matrix-jobs:
    needs: model-autogen-op-tests
    runs-on: ubuntu-latest
    env:
      pytest_verbosity: 0
    steps:
      - uses: actions/checkout@v4
      - uses: ./.github/actions/common_repo_setup
      - name: Download All Input Variations Metrics Artifacts
        uses: actions/download-artifact@v4
        with:
          pattern: model-autogen-op-tests-metrics-group-*
          merge-multiple: true
          path: metrics-autogen-op/

      - name: Upload Input Variations Metrics Artifact
        uses: actions/upload-artifact@v4
        with:
          name: model-autogen-op-tests-metrics
          path: metrics-autogen-op/

  collect-metrics:
    needs: [model-autogen-op-tests]
    if: ${{ github.event_name == 'workflow_dispatch' && inputs.commit_report != 'None'}}
    runs-on: ["in-service"]
    container: 
      image: ${{ inputs.docker_tag }}
      credentials:
        username: ${{ github.actor }}
        password: ${{ secrets.GH_TOKEN }}
      options: >-
        --rm -v /dev/hugepages-1G:/dev/hugepages-1G --device /dev/tenstorrent
        -v ${{ github.workspace }}:${{ github.workspace }} -w ${{ github.workspace }}
    env:
      PYTHONPATH: ${{ github.workspace }}
    steps:
      - uses: actions/checkout@v4
      - uses: ./.github/actions/common_repo_setup
      - name: Download All Metrics Artifacts
        uses: actions/download-artifact@v4
        with:
          pattern: model-tests-metrics-group-*
          merge-multiple: true
          path: metrics/

      - name: Download All Input Variations Metrics Artifacts
        uses: actions/download-artifact@v4
        with:
          pattern: model-autogen-op-tests-metrics-group-*
          merge-multiple: true
          path: metrics-autogen-op/
      # - name: Download Metrics Artifacts
      #   env:
      #     GH_TOKEN: ${{ secrets.GH_TOKEN }}
      #     GH_REPO: ${{ github.repository }}
      #   run: |
      #     mkdir -p metrics
      #     for i in $(seq 1 40); do
      #       echo "Downloading metrics for Group $i"
      #       gh run download ${{ github.run_id }} --name "model-tests-metrics-group-$i" --dir metrics/
      #       cd metrics
      #       ls -l
      #       cd ..
      #     done

      - name: Collect Metrics Report
        run: |
          python3 tools/collect_metrics.py
          git config user.name "GitHub Actions"
          git config user.email "actions@github.com"
          git pull

          if [ "${{ inputs.commit_report }}" == "All" ]; then
            git add --all
          elif [ "${{ inputs.commit_report }}" == "Docs" ]; then
            git add README.md
            git add docs/
          else
            echo "No files will be committed"
            exit 0
          fi

          if git diff --cached --quiet; then
            echo "No changes to commit"
            exit 0
          else
            git commit -m '[auto][on-merge-queue] Update metrics report in README.md'
            git push
          fi
          
          python3 tools/send_to_data_team.py \
              --github_workflow_id ${{ github.run_id }} \
              --sftp_host ${{ secrets.SFTP_OPTEST_HOST }} \
              --sftp_user ${{ secrets.SFTP_OPTEST_USER }} \
              --sftp_private_key ${{ secrets.SFTP_OPTEST_PRIVATE_KEY }}

  tests-passed:
    if: ${{ always() }}
    outputs:
      didpass: ${{ steps.check.outputs.didpass }}
    runs-on: ubuntu-latest    
    needs: [model-tests, tools-tests, lowering-tests, collect-metrics]
    steps:
      - id: check
        run: |
          model_result="${{ needs.model-tests.result}}"
          tools_result="${{ needs.tools-tests.result}}"
          lowering_result="${{ needs.lowering-tests.result}}"
          if [[ ($tools_result == "success" || $tools_result == "skipped") && 
                ($lowering_result == "success" || $lowering_result == "skipped") && 
                ($model_result == "success" || $model_result == "skipped") ]] ; then
            echo "didpass=0" >> $GITHUB_OUTPUT
            exit 0
          else
            echo "didpass=1" >> $GITHUB_OUTPUT
            exit 1
          fi
